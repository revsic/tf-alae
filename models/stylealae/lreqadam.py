import tensorflow as tf
from tensorflow.python.framework import ops
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import control_flow_ops
from tensorflow.python.ops import math_ops
from tensorflow.python.ops import state_ops


class LrEqAdam(tf.keras.optimizers.Optimizer):
    """Learning rate equalized Adam optimizer.
    It use lreq_coeff instead of estimating gradient with beta1.
    In module lreq, Dense, Conv2d and Conv2dTranpose are implemented for lreq adam,
    which define the lreq_coeff with standard deviation of their computed outputs.
    """
    def __init__(self,
                 learning_rate=1e-3,
                 beta_2=0.999,
                 epsilon=1e-7,
                 name='LrEqAdam',
                 **kwargs):
        super(LrEqAdam, self).__init__(name)
        # set hyper parameters
        self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))
        self._set_hyper('decay', self._initial_decay)
        self._set_hyper('beta_2', beta_2)
        self.epsilon = epsilon

    def _create_slots(self, var_list):
        """Create states for second gradient moments.
        Args:
            var_list: list of trainable variables.
        """
        for var in var_list:
            self.add_slot(var, 'v')

    def set_weights(self, weights):
        """If the weights are generated by Keras V1 optimizer, it includes vhats
        even without amsgrad, i.e, V1 optimizer has 3x + 1 variables, while V2
        optimizer has 2x + 1 variables. Filter vhats out for compatibility.
        """
        params = self.weights
        num_vars = int((len(params) - 1) / 2)
        if len(weights) == 3 * num_vars + 1:
            weights = weights[:len(params)]
        super(LrEqAdam, self).set_weights(weights)

    def _resource_apply_dense(self, grad, var, apply_state=None):
        """Update trainable variables which stored as dense matrix.
        Args:
            grad: gradient tensor.
            var: variable tensor.
            apply_state: deprecated.
        Returns:
            assigning operations for updating variables.
        """
        var_dtype = var.dtype.base_dtype
        lr_t = self._decayed_lr(var_dtype)
        local_step = math_ops.cast(self.iterations + 1, var_dtype)
        beta_2_t = array_ops.identity(self._get_hyper('beta_2', var_dtype))
        beta_2_power = math_ops.pow(beta_2_t, local_step)
        epsilon_t = ops.convert_to_tensor_v2(self.epsilon, var_dtype)

        lr_t = lr_t * math_ops.sqrt(1 - beta_2_power)

        v = self.get_slot(var, 'v')
        # update second moments
        v_t = state_ops.assign(
            v,
            beta_2_t * v + (1. - beta_2_t) * math_ops.square(grad),
            use_locking=self._use_locking)

        var_delta = 1. / (math_ops.sqrt(v_t) + epsilon_t)
        # use lreq_coeff instead of first moments
        if hasattr(var, 'lreq_coeff'):
            lreq_coeff = ops.convert_to_tensor_v2(var.lreq_coeff, var_dtype)
            var_delta = var_delta * lreq_coeff
        # new variables
        var_t = math_ops.sub(var, lr_t * var_delta)
        # update operation
        var_update = state_ops.assign(var, var_t, use_locking=self._use_locking)
        return var_update

    def _resource_apply_sparse(self, grad, var, indices, apply_state=None):
        raise NotImplementedError('sparse gradient is not implemented on LrEqAdam')

    def get_config(self):
        config = super(LrEqAdam, self).get_config()
        config.update({
            'learning_rate': self._serialize_hyperparameter('learning_rate'),
            'decay': self._serialize_hyperparameter('decay'),
            'beta_2': self._serialize_hyperparameter('beta_2'),
            'epsilon': self.epsilon
        })
        return config
